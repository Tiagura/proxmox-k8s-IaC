# ------------------------------------------------
# Preflight | Validate inventory topology
# ------------------------------------------------
- name: Preflight | Validate inventory topology
  hosts: localhost
  gather_facts: false
  vars:
    embedded_ha_control_plane: "{{ lookup('env', 'embedded_ha_control_plane') | default('false') | bool }}"

    master_count: "{{ groups['masters'] | default([]) | length }}"
    worker_count: "{{ groups['workers'] | default([]) | length }}"
    loadbalancer_count: "{{ groups['loadbalancers'] | default([]) | length }}"

  tasks:
    - name: Show detected topology
      debug:
        msg:
          - "Masters: {{ master_count }}"
          - "Workers: {{ worker_count }}"
          - "LoadBalancers: {{ loadbalancer_count }}"
          - "Embedded HA Control Plane: {{ embedded_ha_control_plane }}"

    - name: Fail if embedded HA is enabled while external load balancers exist
      fail:
        msg: |
          INVALID CONFIGURATION:
          Embedded HA control plane is ENABLED, but load balancer nodes are defined.

          This configuration is not supported because:
          - Embedded HA runs HAProxy + Keepalived on MASTER nodes
          - External load balancers would be ignored

          Choose one of the following:
          - Disable embedded HA (embedded_ha_control_plane=false)
          - Remove load balancer nodes from the inventory

          Aborting.
      when:
        - embedded_ha_control_plane
        - loadbalancer_count | int > 0

    - name: HA sanity check
      pause:
        prompt: |
          Detected configuration:
          - Masters: {{ master_count }}
          - Load Balancers: {{ loadbalancer_count }}
          - Embedded HA Control Plane: {{ embedded_ha_control_plane }}
          - VIP Address: {{ vip_address | default('NOT SET') }}

          {% if embedded_ha_control_plane and master_count | int == 1 %}
          WARNING:
          Embedded HA is enabled but only ONE master node exists.
          This provides NO real high availability.
          {% endif %}

          {% if (master_count | int > 1)
                and (embedded_ha_control_plane or loadbalancer_count | int > 1)
                and vip_address | default('') | trim == '' %}
          WARNING:
          HA is configured WITHOUT a VIP address.
          Failover will NOT be seamless.
          {% endif %}

          Do you want to continue with this configuration? (yes/no)
      register: ha_sanity_confirm
      when: >
        (embedded_ha_control_plane and master_count | int <= 1)
        or (
          (master_count | int > 1)
          and (embedded_ha_control_plane or loadbalancer_count | int > 1)
          and vip_address | default('') | trim == ''
        )

    - name: Abort if HA sanity check was rejected
      fail:
        msg: "Aborted by user after HA sanity check."
      when:
        - ha_sanity_confirm is defined
        - ha_sanity_confirm.user_input is defined
        - ha_sanity_confirm.user_input | lower not in ['y', 'yes']


    - name: Fail if no master nodes are defined
      fail:
        msg: |
          ERROR: No master nodes were found in the inventory. At least one master is required to bootstrap a Kubernetes cluster.
      when: (master_count | int) < 1

    - name: Warn if no worker nodes are defined
      pause:
        prompt: |
          WARNING: No worker nodes were found in the inventory.

          This will result in a control-plane-only cluster.

          Do you want to continue? (yes/no)
      register: no_workers_confirm
      when: (worker_count | int) == 0

    - name: Abort if user does not want to continue without workers
      fail:
        msg: "Aborted by user: no worker nodes present."
      when:
        - (worker_count | int) == 0
        - no_workers_confirm.user_input | lower not in ['y', 'yes']

    - name: Warn about multi-master cluster without HA control plane
      pause:
        prompt: |
          WARNING: You have {{ master_count }} master nodes and NO load balancers.

          This means:
          - No external HA control plane endpoint
          - API server will NOT be highly available

          You may have intended to:
          - Enable embedded HA control plane
          - Or add load balancer nodes

          Did you miss something?
          Do you want to continue anyway? (yes/no)
      register: no_ha_confirm
      when:
        - (master_count | int) > 1
        - (loadbalancer_count | int) == 0
        - not embedded_ha_control_plane

    - name: Abort if user does not want to continue without HA control plane
      fail:
        msg: "Aborted by user: multi-master without HA control plane."
      when:
        - (master_count | int) > 1
        - (loadbalancer_count | int) == 0
        - not embedded_ha_control_plane
        - no_ha_confirm.user_input | lower not in ['y', 'yes']

    - name: Determine if HA control plane should be enabled
      set_fact:
        ha_control_plane_enabled: >-
          {{ embedded_ha_control_plane and (master_count | int) > 1 }}

    - name: Debug HA control plane decision
      debug:
        msg: "HA control plane enabled: {{ ha_control_plane_enabled }}"

# =========================================
# Preflight | Determine HA target group
# =========================================
- name: Preflight | Determine HAProxy / Keepalived target group
  hosts: localhost
  gather_facts: false
  tasks:
    - name: Determine HA target group
      set_fact:
        ha_target_group: >-
          {{ 'masters' if ha_control_plane_enabled else 'loadbalancers' }}
      delegate_facts: true

    - name: Debug selected HA target group
      debug:
        msg: "HA target group determined: {{ ha_target_group }}"

# ==================================================
# Preflight | Select bootstrap master and controlPlaneEndpoint
# ==================================================
- name: Preflight | Determine bootstrap master and control plane endpoint
  hosts: localhost
  gather_facts: false
  vars:
    embedded_ha_control_plane: "{{ lookup('env', 'embedded_ha_control_plane') | default('false') | bool }}"
    control_plane_endpoint_env: "{{ control_plane_endpoint | default(lookup('env', 'control_plane_endpoint') | default('')) | trim }}"
    vip_address: "{{ lookup('env', 'vip_address') | default('') | trim }}"
    control_plane_port: "{{ lookup('env', 'control_plane_endpoint_port') | default('6443') }}"
    pod_subnet: "{{ lookup('env', 'pod_subnet') | default('10.32.0.0/16') }}"

  tasks:
    - name: Ensure at least one master exists
      fail:
        msg: |
          ERROR:
          No master nodes defined in inventory.
          At least one master is required to bootstrap the cluster.
      when: groups['masters'] | default([]) | length < 1

    - name: Select bootstrap master
      set_fact:
        bootstrap_master: "{{ groups['masters'][0] }}"

    - name: Show bootstrap master
      debug:
        msg: "Bootstrap master: {{ bootstrap_master }}"

    - name: Resolve master IPs
      set_fact:
        master_ips: "{{ groups['masters'] | map('extract', hostvars, 'ansible_host') | list }}"

    - name: Resolve HA IPs (LBs or embedded masters)
      set_fact:
        ha_ips: "{{ master_ips if embedded_ha_control_plane else
                  (groups['loadbalancers'] | default([]) |
                    map('extract', hostvars, 'ansible_host') |
                    map('trim') | list) }}"


    # ------------------------------------------------
    # Determine controlPlaneEndpoint
    # ------------------------------------------------
    # 1. If control_plane_endpoint is already passed trough the environment variable, use it.
    # 2. If there is at least 2 load balancers, use the passed vip_address as control_plane_endpoint.
    # 3. If there is only one load balancer, use its IP address as control_plane_endpoint.
    # 4. If there are no load balancers, use the first master node IP address as control_plane_endpoint.
    - name: Determine control plane endpoint
      set_fact:
        control_plane_endpoint_final: >-
          {% if control_plane_endpoint_env | trim != '' %}
            {{ control_plane_endpoint_env | trim }}
          {% elif ha_ips | length > 1 and vip_address | trim != '' %}
            {{ vip_address | trim }}
          {% elif ha_ips | length == 1 %}
            {{ ha_ips[0] }}
          {% else %}
            {{ master_ips[0] }}
          {% endif %}

    - name: Debug Configuration Information
      debug:
        msg: |
          Bootstrap Master: {{ bootstrap_master }}
          Master IPs: {{ master_ips }}
          HA IPs (LBs or embedded masters): {{ ha_ips }}
          Embedded HA Control Plane: {{ embedded_ha_control_plane }}
          VIP Address: {{ vip_address }}
          Control Plane Endpoint (env): {{ control_plane_endpoint_env }}
          Selected Control Plane Endpoint: {{ control_plane_endpoint_final }}:{{ control_plane_port }}

# ==================================================
# Bootstrap | General Packages Installation
# ==================================================
- name: Bootstrap | General Packages Installation
  hosts: all
  roles:
    - bootstrap-node

# =========================================
# Bootstrap | K8s Packages Installation
# =========================================
- name: Bootstrap | K8s Packages Installation
  hosts:
    - masters
    - workers
  roles:
    - install-runc
    - install-cni
    - install-containerd
    - configure-containerd
    - install-crictl
    - install-kubeadm-kubelet
    - install-kubectl

# =========================================
# HA Control Plane Setup | Install and Configure Proxies
# =========================================
- name: HA Control Plane Setup | Install and Configure Proxies
  hosts: "{{ hostvars['localhost'].ha_target_group }}"
  gather_facts: false
  vars:
    ha_nodes: "{{ groups[hostvars['localhost'].ha_target_group] }}"
  tasks:

    - name: Set HA role and Keepalived priority
      set_fact:
        haproxy_role: >-
          {{ ('single' if ha_nodes | length == 1
             else 'master' if inventory_hostname == ha_nodes[0]
             else 'slave') | trim }}

        keepalived_priority: >-
          {{ (150 - (10 * ha_nodes.index(inventory_hostname))
             if ha_nodes | length > 1 else 0) }}

    - name: Debug HA role and Keepalived priority
      debug:
        msg: |
          Host: {{ inventory_hostname }}
          HAProxy Role: {{ haproxy_role }}
          Keepalived Priority: {{ keepalived_priority }}

    - name: Install HAProxy and Keepalived prerequisites
      include_role:
        name: install-loadbalancer-packages

    # -------------------------------------------------
    # External LB mode configure HAProxy
    # -------------------------------------------------
    - name: Configure HAProxy
      include_role:
        name: configure-haproxy
      when: not hostvars['localhost'].ha_control_plane_enabled

    # -------------------------------------------------
    # Embedded HA or External LB configure Keepalived
    # -------------------------------------------------
    - name: Configure Keepalived master role
      include_role:
        name: configure-keepalived-master
      when: haproxy_role == 'master'

    - name: Configure Keepalived slave role
      include_role:
        name: configure-keepalived-slave
      when: haproxy_role == 'slave'

# =========================================
# General | Reboot all devices 
# =========================================
- name: General | Reboot all devices
  hosts: all
  tasks:
    - name: Reboot all devices
      ansible.builtin.reboot: 
        msg: "Rebooting all devices"
        reboot_timeout: 600
      become: true

# =========================================
# Cluster Setup | Bootstrap initial Master Node
# =========================================
- name: Cluster Setup | Bootstrap initial Master Node
  hosts: masters
  vars_files:
    - roles/install-kubeadm-kubelet/defaults/main.yml
  vars:
    control_plane_endpoint: "{{ control_plane_endpoint_final | trim }}"
  tasks:
    - name: Run cluster initialization on the first master
      include_role:
        name: initialize-cluster-kubeadm
      when: inventory_hostname == groups['masters'][0]

# =========================================
# Cluster Setup | Copy kubeconfig to Setup Machine
# =========================================
- name: Cluster Setup | Copy kubeconfig to Setup Machine
  hosts: localhost
  tasks:
    - name: Ensure .kube directory exists on Ansible control machine
      ansible.builtin.file:
        path: "{{ ansible_env.HOME }}/.kube"
        state: directory
        mode: '0755'

    - name: Fetch /etc/kubernetes/admin.conf to control machine
      ansible.builtin.fetch:
        src: /etc/kubernetes/admin.conf
        dest: "{{ lookup('env', 'HOME') }}/.kube/config"
        flat: true
      delegate_to: "{{ groups['masters'][0] }}"
      run_once: true
      become: true

    # Set permissions for the kubeconfig file ->  sudo chown $(id -u):$(id -g) $HOME/.kube/config
    - name: Set permissions for kubeconfig file
      ansible.builtin.file:
        path: "{{ lookup('env', 'HOME') }}/.kube/config"
        owner: "{{ ansible_user_id }}"
        group: "{{ ansible_user_gid }}"
        mode: '0644'

# =========================================
# Cluster Setup | Join remaining masters
# =========================================
- name: Cluster Setup | Join remaining masters
  hosts: masters
  become: true
  vars:
    bootstrap_master: "{{ hostvars['localhost'].bootstrap_master }}"
    control_join_command: >-
      {{ hostvars[bootstrap_master].kubeadm_join_command_control_plane | default('') }}
  tasks:
    - name: Join master to the control plane
      include_role:
        name: master-join-cluster-kubeadm
      when:
        - inventory_hostname != bootstrap_master
        - control_join_command != ''

# =========================================
# Cluster Setup | Join worker nodes
# =========================================
- name: Cluster Setup | Join worker nodes
  hosts: workers
  become: true
  vars:
    bootstrap_master: "{{ hostvars['localhost'].bootstrap_master }}"
    worker_join_command: >-
      {{ hostvars[bootstrap_master].kubeadm_join_command_worker | default('') }}
  tasks:
    - name: Join worker to the cluster
      include_role:
        name: worker-join-cluster-kubeadm
      when:
        - worker_join_command != ''
